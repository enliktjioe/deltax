{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 75%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 75%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 686D-8B85\n",
      "\n",
      " Directory of C:\\Users\\handy89\\deltax\\RCSnail-AI-lite\\src\n",
      "\n",
      "01/09/2021  07:01 PM    <DIR>          .\n",
      "01/09/2021  07:01 PM    <DIR>          ..\n",
      "01/09/2021  03:25 PM    <DIR>          .ipynb_checkpoints\n",
      "01/09/2021  06:47 PM             2,389 bangke.jpg\n",
      "01/09/2021  01:34 PM             3,620 clean_json_asRCSnail_masked.py\n",
      "01/09/2021  01:55 PM            17,538 example_input_in_try_loop.png\n",
      "12/22/2020  06:03 PM    <DIR>          learning\n",
      "12/22/2020  03:19 PM             3,051 main.py\n",
      "12/23/2020  12:49 PM             6,679 main_old.py\n",
      "12/22/2020  03:19 PM             6,164 main_ubuntu.py\n",
      "01/09/2021  01:54 PM             7,171 main_windows.py\n",
      "12/22/2020  05:52 PM         5,864,112 new_0.13_0.21.h5\n",
      "01/09/2021  06:15 PM    <DIR>          preprocessed\n",
      "01/09/2021  01:55 PM            23,499 raw_input_in_try_loop.png\n",
      "01/09/2021  07:01 PM            31,115 training_ground-handy.ipynb\n",
      "12/22/2020  06:03 PM    <DIR>          utilities\n",
      "              10 File(s)      5,965,338 bytes\n",
      "               6 Dir(s)  273,918,906,368 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from commons.configuration_manager import ConfigurationManager\n",
    "#from src.utilities.transformer import Transformer\n",
    "from src.learning.training.generator import Generator, GenFiles\n",
    "#from src.learning.models import create_standalone_nvidia_cnn, create_standalone_resnet\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(title, plot_elem, figsize=(18, 10)):\n",
    "    fig=plt.figure(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    #x = np.arange(0, len(plot_elems[0]['data']), 1)\n",
    "    \n",
    "    #for plot_elem in plot_elems:\n",
    "    #    plt.errorbar(x, plot_elem['data'], yerr=plot_elem['error'], label=plot_elem['label'], alpha=plot_elem['alpha'], fmt='-o', capsize=5)\n",
    "    \n",
    "    plt.plot(list(range(1,len(plot_elem['data'])+1)), plot_elem['data'])\n",
    "    plt.grid(axis='both')\n",
    "    #plt.legend(loc='best', prop={'size': 15})\n",
    "    plt.show()\n",
    "    plt.savefig('./' + title + '.png')\n",
    "    \n",
    "def get_model_num(model_path, model_prefix):\n",
    "    model_files = [fn for fn in os.listdir(model_path) if fn.startswith(model_prefix) and fn.endswith('.h5') and 'dagger_' not in fn]\n",
    "    # expected format is \"model_n1_m1_9.h5\"\n",
    "    existing_nums = [int(fn.split('_')[3].split('.')[0]) for fn in model_files]\n",
    "    \n",
    "    if len(existing_nums) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        latest_num = sorted(existing_nums)[-1]\n",
    "        return int(latest_num) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_manager = ConfigurationManager()\n",
    "config = config_manager.config\n",
    "\n",
    "memory = (1, 1)\n",
    "#ARDI' comment -- we just use (1,1) this refers to how many frames are used and sth else, not sure what\n",
    "\n",
    "model_path = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_standalone_nvidia_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ecc41d7bd94f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#ARDI's comment:  Nividia is the model we want to use (resnet might be good to??)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#generator = Generator(config, memory_tuple= memory, base_path='../', batch_size=batch_size, column_mode='all', shuffle_data=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_standalone_nvidia_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m180\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#\"steer and throttle\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_standalone_nvidia_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch>30 and epoch%10==0:\n",
    "        lr*=0.5\n",
    "    return lr\n",
    "\n",
    "\n",
    "# Model experiments\n",
    "import os\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "verbose = 1\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "#in here added option to not shuffle, so last 20% of recording time is used as val set -- in future might want to reduce proportion of val set\n",
    "#generator = Generator(config, memory_tuple= memory, base_path='../', batch_size=batch_size, column_mode='all', shuffle_data=False) \n",
    "\n",
    "#frame_shape, numeric_shape = generator.get_shapes()\n",
    "#tqdm.write('Target shape: {}'.format(diff_shape)) #tqdm is some package that allow to track the progress of operations\n",
    "#tqdm.write('Input shapes: {}; {}'.format(frame_shape, numeric_shape))\n",
    "\n",
    "models = []\n",
    "\n",
    "#ARDI's comment:  Nividia is the model we want to use (resnet might be good to??)\n",
    "#generator = Generator(config, memory_tuple= memory, base_path='../', batch_size=batch_size, column_mode='all', shuffle_data=False) \n",
    "models.append((create_standalone_nvidia_cnn(activation='linear', input_shape=(60, 180, 3), output_shape=2), generator.generate))\n",
    "\n",
    "#\"steer and throttle\"\n",
    "#models.append((create_small_cnn(activation='linear', input_shape=(60, 180, 3), output_shape=2), generator.generate))\n",
    "\n",
    "#generator = Generator(config, memory_tuple= memory, base_path='../', batch_size=batch_size, column_mode='all', shuffle_data=False) \n",
    "#models.append((create_small_cnn(activation='linear', input_shape=(50, 180, 3), output_shape=2), generator.generate))\n",
    "\n",
    "callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler)]\n",
    "\n",
    "for model, generate_method in tqdm(models):\n",
    "    result_desc = 'n{}_m{}'.format(*memory)\n",
    "    tqdm.write(result_desc)\n",
    "\n",
    "    hist = model.fit(generate_method(data='train'),\n",
    "                     steps_per_epoch=generator.train_batch_count,\n",
    "                     validation_data=generate_method(data='test'),\n",
    "                     validation_steps=generator.test_batch_count,\n",
    "                     callbacks=callbacks,\n",
    "                     epochs=epochs, verbose=verbose)\n",
    "\n",
    "    model_file_prefix = 'model_n{}_m{}'.format(*memory)\n",
    "    model_file_suffix = '_{}.{}'\n",
    "\n",
    "    model_number = get_model_num(model_path, model_file_prefix)\n",
    "    plot_model(model, to_file=model_path + model_file_prefix + model_file_suffix.format(model_number, 'png'), show_shapes=True)\n",
    "    model.save(model_path + model_file_prefix + model_file_suffix.format(model_number, 'h5'))\n",
    "    \n",
    "    current_loss = hist.history['loss']\n",
    "    current_val_loss = hist.history['val_loss'] \n",
    "    \n",
    "    losses.append(current_loss)\n",
    "    print(val_losses)\n",
    "    val_losses.append(current_val_loss)\n",
    "    \n",
    "    tqdm.write(\"Loss per epoch: {}\".format(current_loss))\n",
    "    tqdm.write(\"Validation loss per epoch: {}\".format(current_val_loss))\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "os.system(\"printf '\\a'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_losses)\n",
    "print(hist.history['val_loss'] )\n",
    "loss_data = []\n",
    "val_loss_data = []\n",
    "\n",
    "\n",
    "val_loss_data = {'data': hist.history['val_loss'], 'label': 'Validation loss', 'alpha': 1.0}\n",
    "plot_stuff('Nivida cnn standalone validation loss', val_loss_data, figsize=(10, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = {'data': losses[0], 'label': 'Loss', 'alpha': 1.0}\n",
    "plot_stuff('Training', loss_data, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo load test set into memeory, evaluate\n",
    "import keras\n",
    "model = keras.models.load_model(\"../masked_test.h5\")\n",
    "import glob\n",
    "\n",
    "val_data_loc=\"../n1_m1\"\n",
    "filenames = glob.glob(\"../n1_m1/*\")\n",
    "print(len(filenames))\n",
    "nr_of_datapoints = int(len(filenames)/2) #label and image files\n",
    "\n",
    "\n",
    "MAEs=[]\n",
    "preds=[]\n",
    "labels=[]\n",
    "\n",
    "for batch in range(nr_of_datapoints//32-32,nr_of_datapoints//32): # using the end of file. 32 batches of size batch of 32\n",
    "    frames=np.zeros((32,50,180,3))\n",
    "    commands = np.zeros((32,2))\n",
    "    for i in range(32):\n",
    "        frames[i,:] = np.load(\"../n1_m1/frame_\"+str(batch*32+i).zfill(7)+\".npy\")\n",
    "        commands[i] = np.load(\"../n1_m1/commands_\"+str(batch*32+i).zfill(7)+\".npy\")\n",
    "    MAEs.append(model.evaluate(frames,commands, batch_size=32))\n",
    "    pred = model.predict(frames)\n",
    "    preds.append(pred)\n",
    "    labels.append(commands)\n",
    "    \n",
    "print(np.mean(MAEs))\n",
    "#TODO MAE for steer and throttle separately, mased on preds and commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(MAEs))\n",
    "p_steer=np.array(preds)[:,:,0]\n",
    "l_steer=np.array(labels)[:,:,0]\n",
    "p_throttle=np.array(preds)[:,:,1]\n",
    "l_throttle=np.array(labels)[:,:,1]\n",
    "\n",
    "print(p_steer.shape,l_steer.shape)\n",
    "p_steer=p_steer.flatten()\n",
    "l_steer=l_steer.flatten()\n",
    "p_throttle=p_throttle.flatten()\n",
    "l_throttle=l_throttle.flatten()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(32,8))\n",
    "plt.plot(range(1000),l_steer[:1000],label=\"human steering\")\n",
    "plt.plot(range(1000),p_steer[:1000],label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(32,8))\n",
    "plt.plot(range(1000),l_throttle[:1000],label=\"human throttle\")\n",
    "plt.plot(range(1000),p_throttle[:1000],label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(l_steer,p_steer,s=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import skimage\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed/cleaned_all\\commands_0000001.npy\n",
      "18454\n",
      "18454\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "val_data_loc=\"preprocessed/cleaned_all\"\n",
    "filenames = glob.glob(\"preprocessed/cleaned_all/*\")\n",
    "print(filenames[1])\n",
    "print(len(filenames))\n",
    "nr_of_datapoints = int(len(filenames)) #label and image files\n",
    "print(nr_of_datapoints)\n",
    "\n",
    "\n",
    "MAEs=[]\n",
    "preds=[]\n",
    "labels=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'preprocessed/cleaned_all/frame_0009227.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3b9a9afc0812>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#commands = np.zeros((1,2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"preprocessed/cleaned_all/frame_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mcommands\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"preprocessed/cleaned_all/commands_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\handy89\\miniconda3\\envs\\deltax\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'preprocessed/cleaned_all/frame_0009227.npy'"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "commands = []\n",
    "for batch in range(1,nr_of_datapoints): # using the end of file. 32 batches of size batch of 32\n",
    "    #frames=np.zeros((1,60,180,3))\n",
    "    #commands = np.zeros((1,2))\n",
    "    \n",
    "    frames.append(np.load(\"preprocessed/cleaned_all/frame_\"+str(batch).zfill(7)+\".npy\"))\n",
    "    commands.append(np.load(\"preprocessed/cleaned_all/commands_\"+str(batch).zfill(7)+\".npy\"))\n",
    "    \n",
    "    \n",
    "    #print(commands)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #MAEs.append(model.evaluate(frames,commands, batch_size=32))\n",
    "    #pred = model.predict(frames)\n",
    "    #preds.append(pred)\n",
    "    #labels.append(commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9226, 60, 180, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frames = frames.reshape(1,60,180,3)\n",
    "frames = np.array(frames)\n",
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9226, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#commands = commands.reshape(1, 2)\n",
    "commands = np.array(commands)\n",
    "commands.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 60, 180, 3)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 30, 90, 24)        1824      \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 15, 45, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 8, 23, 48)         43248     \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 8, 23, 64)         27712     \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 8, 23, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 11776)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1164)              13708428  \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               116500    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 13,961,858\n",
      "Trainable params: 13,961,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_standalone_nvidia_cnn(activation='linear', input_shape=(60, 180, 3), output_shape=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7380 samples, validate on 1846 samples\n",
      "Epoch 1/10\n",
      "7380/7380 [==============================] - 62s 8ms/sample - loss: 2.1374 - val_loss: 1.4468\n",
      "Epoch 2/10\n",
      "7380/7380 [==============================] - 59s 8ms/sample - loss: 1.3375 - val_loss: 1.3112\n",
      "Epoch 3/10\n",
      "7380/7380 [==============================] - 63s 8ms/sample - loss: 1.2384 - val_loss: 1.2287\n",
      "Epoch 4/10\n",
      "7380/7380 [==============================] - 61s 8ms/sample - loss: 1.1436 - val_loss: 1.1696\n",
      "Epoch 5/10\n",
      "7380/7380 [==============================] - 62s 8ms/sample - loss: 1.0838 - val_loss: 1.0954\n",
      "Epoch 6/10\n",
      "7380/7380 [==============================] - 59s 8ms/sample - loss: 1.0286 - val_loss: 1.0375\n",
      "Epoch 7/10\n",
      "7380/7380 [==============================] - 56s 8ms/sample - loss: 0.9748 - val_loss: 0.9910\n",
      "Epoch 8/10\n",
      "7380/7380 [==============================] - 59s 8ms/sample - loss: 0.9294 - val_loss: 1.0140\n",
      "Epoch 9/10\n",
      "7380/7380 [==============================] - 58s 8ms/sample - loss: 0.8868 - val_loss: 0.9180\n",
      "Epoch 10/10\n",
      "7380/7380 [==============================] - 57s 8ms/sample - loss: 0.8411 - val_loss: 0.9154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e0145394c8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(frames, commands, batch_size=64, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sama\n"
     ]
    }
   ],
   "source": [
    "mem_frame = frames[10].reshape(1,60,180,3)\n",
    "mem_frame.shape\n",
    "mem_frame\n",
    "\n",
    "new_mem_frame = mem_frame\n",
    "\n",
    "if(new_mem_frame.all() == mem_frame.all()):\n",
    "    print(\"sama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02560089, 0.42005342]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(mem_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(model, to_file=model_path + model_file_prefix + model_file_suffix.format(model_number, 'png'), show_shapes=True)\n",
    "model.save('model_team_3_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standalone_nvidia_cnn(activation='linear', input_shape=(60, 180, 3), output_shape=1):\n",
    "    \"\"\"\n",
    "    Activation: linear, softmax.\n",
    "    Architecture is from nvidia paper mentioned in https://github.com/tanelp/self-driving-convnet/blob/master/train.py\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.layers import Convolution2D\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras.layers import Flatten\n",
    "    from tensorflow.keras.layers import Input\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.losses import mean_squared_error, mean_absolute_error\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    conv_1 = Convolution2D(24, kernel_size=(5, 5), kernel_regularizer=l2(0.0005), strides=(2, 2), padding=\"same\", activation=\"elu\")(inputs)\n",
    "    conv_2 = Convolution2D(36, kernel_size=(5, 5), kernel_regularizer=l2(0.0005), strides=(2, 2), padding=\"same\", activation=\"elu\")(conv_1)\n",
    "    conv_3 = Convolution2D(48, kernel_size=(5, 5), kernel_regularizer=l2(0.0005), strides=(2, 2), padding=\"same\", activation=\"elu\")(conv_2)\n",
    "    conv_4 = Convolution2D(64, kernel_size=(3, 3), kernel_regularizer=l2(0.0005), padding=\"same\", activation=\"elu\")(conv_3)\n",
    "    conv_5 = Convolution2D(64, kernel_size=(3, 3), kernel_regularizer=l2(0.0005), padding=\"same\", activation=\"elu\")(conv_4)\n",
    "    flatten = Flatten()(conv_5)\n",
    "    dense_1 = Dense(1164, kernel_regularizer=l2(0.0005), activation=\"elu\")(flatten)\n",
    "    dense_2 = Dense(100, kernel_regularizer=l2(0.0005), activation=\"elu\")(dense_1)\n",
    "    dense_3 = Dense(50, kernel_regularizer=l2(0.0005), activation=\"elu\")(dense_2)\n",
    "    dense_4 = Dense(10, kernel_regularizer=l2(0.0005), activation=\"elu\")(dense_3)\n",
    "    out_dense = Dense(output_shape, activation=activation)(dense_4)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=out_dense)\n",
    "    optimizer = Adam(lr=3e-4)\n",
    "    model.compile(loss=mean_absolute_error, optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
